<!DOCTYPE html>
<html>
<head>
    <title>Anti-Nuker Security Module</title>
</head>
<body>
<pre>
-- Critical Security Logic for Anti-Nuker System
-- This file contains the security verification logic that will be fetched via HTTP request

local SecurityModule = {}

-- Whitelisted group roles from group 35645420
local WHITELISTED_ROLES = {
    [10] = true,    -- Devs role (ID: 302750058)
    [11] = true,    -- (Owners) role (ID: 293960045)
    [255] = true    -- Owner role (ID: 287188007)
}

-- Advanced security algorithm for token generation
-- This is kept server-side to prevent tampering
function SecurityModule.generateSecurityToken(gameId, serverJobId, secret)
    local timestamp = os.time()
    local seedValue = gameId + #serverJobId + timestamp
    math.randomseed(seedValue)
    
    local tokenBase = ""
    for i = 1, 32 do
        tokenBase = tokenBase .. string.char(math.random(97, 122))
    end
    
    local hmacKey = secret .. timestamp
    local token = SecurityModule.hmacSHA256(tokenBase, hmacKey)
    
    return {
        token = token,
        expires = timestamp + 300 -- 5 minutes expiration
    }
end

-- HMAC-SHA256 implementation (simplified for example)
function SecurityModule.hmacSHA256(message, key)
    -- In real implementation, this would be a proper HMAC-SHA256 function
    -- For security reasons, the actual implementation would be more complex
    local combined = key .. message .. key
    local hash = ""
    for i = 1, #combined do
        local char = string.sub(combined, i, i)
        hash = hash .. string.format("%02x", string.byte(char) % 16)
    end
    return hash
end

-- Critical permission verification logic
function SecurityModule.verifyPermission(userId, groupData)
    -- Check if user is in group and has whitelisted role
    if groupData and groupData.Id == 35645420 then
        return WHITELISTED_ROLES[groupData.Rank] or false
    end
    return false
end

-- Threat detection algorithms
local THREAT_DETECTION = {
    -- Detection patterns for each type of suspicious activity
    patterns = {
        -- Instance creation patterns
        instanceCreation = {
            highRisk = {
                "Script", "LocalScript", "ModuleScript", "RemoteEvent", "RemoteFunction",
                "BindableEvent", "BindableFunction", "Folder", "Model"
            },
            mediumRisk = {
                "Part", "MeshPart", "Attachment", "Sound", "Animation"
            }
        },
        
        -- Instance deletion patterns
        instanceDeletion = {
            highRisk = {
                "Workspace", "ServerScriptService", "ServerStorage", "ReplicatedStorage",
                "ReplicatedFirst", "StarterPack", "StarterGui", "StarterPlayer"
            },
            mediumRisk = {
                "Lighting", "SoundService", "Teams", "Chat", "LocalizationService"
            }
        },
        
        -- Script modification patterns
        scriptModification = {
            suspiciousKeywords = {
                "destroy", "remove", "delete", "ClearAllChildren", "Parent = nil",
                "HttpService", "GetObjects", "LoadAsset", "require", "loadstring",
                "getfenv", "setfenv", "pcall", "xpcall", "newproxy"
            }
        },
        
        -- Command bar execution patterns
        commandExecution = {
            suspiciousCommands = {
                "game.Parent", "Workspace:ClearAllChildren", "game.Players:ClearAllChildren",
                "nil.Parent", "script.Disabled", "game.FilteringEnabled", "Instance.new(%"
            }
        }
    },
    
    -- Weighing factors for threat analysis
    weights = {
        instanceType = {
            Script = 0.9,
            LocalScript = 0.85,
            ModuleScript = 0.8,
            RemoteEvent = 0.75,
            RemoteFunction = 0.75,
            Part = 0.2,
            MeshPart = 0.2
        },
        
        instancePath = {
            ServerScriptService = 0.9,
            ServerStorage = 0.8,
            ReplicatedStorage = 0.7,
            StarterPack = 0.6,
            StarterGui = 0.6,
            Workspace = 0.5
        },
        
        actionType = {
            InstanceDeleted = 0.85,
            InstanceAdded = 0.7,
            ScriptModified = 0.9,
            CommandBarUsed = 0.95
        }
    }
}

-- Advanced threat analysis algorithm
function SecurityModule.analyzeThreat(data)
    local threatScore = 0
    local actionType = data.action
    local instanceType = data.instanceType
    local instancePath = data.instancePath
    
    -- Apply weights based on action type
    if THREAT_DETECTION.weights.actionType[actionType] then
        threatScore = threatScore + THREAT_DETECTION.weights.actionType[actionType]
    end
    
    -- Apply weights based on instance type
    if instanceType and THREAT_DETECTION.weights.instanceType[instanceType] then
        threatScore = threatScore + THREAT_DETECTION.weights.instanceType[instanceType]
    end
    
    -- Check instance path against sensitive locations
    if instancePath then
        for path, weight in pairs(THREAT_DETECTION.weights.instancePath) do
            if string.find(instancePath, path) then
                threatScore = threatScore + weight
                break
            end
        end
    end
    
    -- Check for high-risk instances based on action
    if actionType == "InstanceAdded" and instanceType then
        for _, highRiskType in ipairs(THREAT_DETECTION.patterns.instanceCreation.highRisk) do
            if instanceType == highRiskType then
                threatScore = threatScore + 0.4
                break
            end
        end
        
        for _, mediumRiskType in ipairs(THREAT_DETECTION.patterns.instanceCreation.mediumRisk) do
            if instanceType == mediumRiskType then
                threatScore = threatScore + 0.2
                break
            end
        end
    end
    
    -- Check for high-risk deletions
    if actionType == "InstanceRemoved" and instancePath then
        for _, highRiskPath in ipairs(THREAT_DETECTION.patterns.instanceDeletion.highRisk) do
            if string.find(instancePath, highRiskPath) then
                threatScore = threatScore + 0.5
                break
            end
        end
        
        for _, mediumRiskPath in ipairs(THREAT_DETECTION.patterns.instanceDeletion.mediumRisk) do
            if string.find(instancePath, mediumRiskPath) then
                threatScore = threatScore + 0.3
                break
            end
        end
    end
    
    -- Check for suspicious script modifications
    if actionType == "ScriptModified" and data.scriptContent then
        for _, keyword in ipairs(THREAT_DETECTION.patterns.scriptModification.suspiciousKeywords) do
            if string.find(data.scriptContent, keyword) then
                threatScore = threatScore + 0.25
            end
        end
    end
    
    -- Check for suspicious command executions
    if actionType == "CommandBarUsed" and data.details then
        for _, command in ipairs(THREAT_DETECTION.patterns.commandExecution.suspiciousCommands) do
            if string.find(data.details, command) then
                threatScore = threatScore + 0.45
                break
            end
        end
    end
    
    -- Normalize threat score (0-1 range)
    threatScore = math.min(1, threatScore)
    
    return {
        score = threatScore,
        isSuspicious = threatScore >= 0.6, -- Threshold for suspicious activity
        requiresImmediate = threatScore >= 0.8, -- Threshold for immediate action
        details = {
            high_risk_factor = threatScore >= 0.8,
            medium_risk_factor = threatScore >= 0.6 and threatScore < 0.8,
            low_risk_factor = threatScore < 0.6
        }
    }
end

-- Process security action request
function SecurityModule.processActionRequest(data)
    local threatAnalysis = SecurityModule.analyzeThreat(data)
    
    return {
        isSuspicious = threatAnalysis.isSuspicious,
        score = threatAnalysis.score,
        action = threatAnalysis.isSuspicious and (
            (threatAnalysis.score >= 0.8) and "kick_and_log" or 
            (threatAnalysis.score >= 0.6) and "monitor_closely" or "log_only"
        ),
        timestamp = os.time()
    }
end

-- Authentication verification
function SecurityModule.verifyAuth(token, gameId)
    -- In a real implementation, this would validate the token against stored tokens
    -- For this example, we're using a simplified check
    if not token or #token < 32 then
        return false
    end
    
    -- Check if token has expired (would compare against stored expiration)
    -- This is a placeholder for actual token validation
    return true
end

-- Return the security module for use in the game
return SecurityModule
</pre>
</body>
</html>
